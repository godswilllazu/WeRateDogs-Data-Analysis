{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9efd137",
   "metadata": {},
   "source": [
    "# Report on Data Wrangling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7299a",
   "metadata": {},
   "source": [
    "Libraries used : \n",
    "\n",
    "    1. Pandas\n",
    "    \n",
    "    2. Numpy\n",
    "    \n",
    "    3. Matplotlib\n",
    "    \n",
    "    4. Tweepy\n",
    "    \n",
    "    5. Requests\n",
    "    \n",
    "    6. Json\n",
    "    \n",
    "    7. SqlAlchemy\n",
    "    \n",
    "    8. Timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4b87b",
   "metadata": {},
   "source": [
    "### Process 1: Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d58a83",
   "metadata": {},
   "source": [
    "The following datasets were gathered:\n",
    "\n",
    "*\ttwitter-archive-enhanced.csv \n",
    "*\tImage-predictions.tsv\n",
    "*\ttweet_json.txt\n",
    "\n",
    "The twitter-archive-enhanced.csv  was downloaded directly from the Udacity Website and loaded into the workspace using **pandas.read_csv**.\n",
    "\n",
    "The image-predictions.tsv file was downloaded programmatically using requests library method and loaded into the workspace using **pandas.read_csv** taking account of the delimeter **sep='\\t'**.\n",
    "\n",
    "The tweet_json.txt file was downloaded using the Tweepy library by Twitter API and loaded via reading each line and then stored in a dataframe.\n",
    "\n",
    "Copies of the twitter-archive-enhanced.csv  and image-predictions.tsv were made after loading each file into the workspace using the **.copy()**  function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6fef8",
   "metadata": {},
   "source": [
    "### Process 2: Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7d7b5",
   "metadata": {},
   "source": [
    "I carried out both visual and programmatic assessments in order to look for both quality and tidiness issues. I documented eight(8) quality issues and three(3) tidiness issues.\n",
    "\n",
    "For the programmatic assessment , I made use of pandas functions such as *.info()   ,   .sample() , .duplicated() , .query and .describe()* as well as checking columns , *dataset[‘column_name’].*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad303ce",
   "metadata": {},
   "source": [
    "### Process 3: Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61628a7c",
   "metadata": {},
   "source": [
    "For this process,  I made sure to solved every listed issue in the assessment stage and also merge the three datasets to one (1).\n",
    "\n",
    "Only original ratings with images (also retweeted set to False) data were assessed and cleaned.\n",
    "\n",
    "All columns were set to their correct data type and re-arranged, missing and incorrect values (e.g ratings) were filled in correctly although they are some missing values which I couldn’t cleaned with the data at hand. \n",
    "\n",
    "I made sure to combine related data columns to one (e.g the dog_types and breeds).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518c490",
   "metadata": {},
   "source": [
    "### Process 4 : Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c33885",
   "metadata": {},
   "source": [
    "I stored the final master dataset as a csv file using pandas   dataset.to_csv() setting index to False. I also stored it in a sql local database (.db) using the sqlite engine of  sqlalchemy library.\n",
    "\n",
    "The master dataset ( twitter_archive_master) has a shape of 1750 * 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
